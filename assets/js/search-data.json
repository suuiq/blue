{
  
    
        "post0": {
            "title": "Title",
            "content": "import numpy as np import matplotlib.pyplot as plt import numpy.random as npr . &#44512;&#51068;&#54620; &#47004;&#45924;&#44050;&#44284; &#44032;&#50864;&#49884;&#50504; &#47004;&#45924; &#48516;&#54252;&#51032; &#52264;&#51060; . x = npr.uniform(size = 1000) #uniform &gt; 균일한 랜덤값 plt.hist(x) plt.title(&#39;Histogram&#39;) plt.legend([&#39;items&#39;]) plt.show() . x = npr.normal(size = 1000) #normal &gt; 가우시안 랜덤 분포값 plt.hist(x) plt.title(&#39;Histogram&#39;) plt.legend([&#39;items&#39;]) plt.show() . import matplotlib.image as mpimg img=mpimg.imread(&#39;9978FB455DB538731B.png&#39;) a = img print(a) plt.imshow(a) . [[[1. 1. 1. 1.] [1. 1. 1. 1.] [1. 1. 1. 1.] ... [1. 1. 1. 1.] [1. 1. 1. 1.] [1. 1. 1. 1.]] [[1. 1. 1. 1.] [1. 1. 1. 1.] [1. 1. 1. 1.] ... [1. 1. 1. 1.] [1. 1. 1. 1.] [1. 1. 1. 1.]] [[1. 1. 1. 1.] [1. 1. 1. 1.] [1. 1. 1. 1.] ... [1. 1. 1. 1.] [1. 1. 1. 1.] [1. 1. 1. 1.]] ... [[1. 1. 1. 1.] [1. 1. 1. 1.] [1. 1. 1. 1.] ... [1. 1. 1. 1.] [1. 1. 1. 1.] [1. 1. 1. 1.]] [[1. 1. 1. 1.] [1. 1. 1. 1.] [1. 1. 1. 1.] ... [1. 1. 1. 1.] [1. 1. 1. 1.] [1. 1. 1. 1.]] [[1. 1. 1. 1.] [1. 1. 1. 1.] [1. 1. 1. 1.] ... [1. 1. 1. 1.] [1. 1. 1. 1.] [1. 1. 1. 1.]]] . &lt;matplotlib.image.AxesImage at 0x7f514adfe610&gt; . npr.uniform &gt; 균등분포의 형태를 따라감 | npr.normal &gt; 정규분포의 형태를 따라감 | . &#49884;&#44033;&#54868; &#51333;&#47448; . 1&#52264;&#50896; . x = npr.normal(size = 1000) plt.hist(x) . (array([ 1., 13., 57., 151., 251., 251., 163., 82., 26., 5.]), array([-3.49331419, -2.80819847, -2.12308275, -1.43796703, -0.75285131, -0.06773559, 0.61738013, 1.30249585, 1.98761157, 2.67272729, 3.35784301]), &lt;BarContainer object of 10 artists&gt;) . x = [&#39;item1&#39;, &#39;item2&#39;, &#39;item3&#39;, &#39;item4&#39;] y = [32, 123, 53, 11] a = plt.pie(y, labels=x, autopct=&#39;%.2f%%&#39;, explode=(0, 0.1, 0, 0)) #autopct &gt; 각 파이의 문자열 출력 형식 설정 / explode &gt; 각 파이를 분리하여 표현하는 정도 plt.show(a) . x = [&#39;item1&#39;, &#39;item2&#39;, &#39;item3&#39;, &#39;item4&#39;] y = [32, 123, 53, 11] plt.bar(x,y) . &lt;BarContainer object of 4 artists&gt; . plt.barh(x,y) . &lt;BarContainer object of 4 artists&gt; . 2&#52264;&#50896; . plt.plot([3, 1, 5, 2]) plt.show() . x1=[2,3,4,5] x2=[3,4,4,5] y1=[5,7,6,7] y2=[1,4,5,9] # 1x2구간으로 나누고 1번째에 그리기 plt.subplot(1, 2, 1) plt.plot(x1, y1) # 1x2구간으로 나누고 2번째에 그리기 plt.subplot(1, 2, 2) plt.plot(x2, y2) plt.show() . x = npr.normal(size=100) y = npr.normal(size=100) plt.scatter(x,y, c=&#39;red&#39;) . &lt;matplotlib.collections.PathCollection at 0x7f8cd227cdf0&gt; . 3&#52264;&#50896; . x = npr.normal(size=100) y = npr.normal(size=100) z= npr.normal(size=100)*400 #반지름의 크기 설정 plt.scatter(x,y,z, c=&#39;violet&#39;) . /home/jsb/anaconda3/lib/python3.8/site-packages/matplotlib/collections.py:922: RuntimeWarning: invalid value encountered in sqrt scale = np.sqrt(self._sizes) * dpi / 72.0 * self._factor . &lt;matplotlib.collections.PathCollection at 0x7f514af24eb0&gt; . import matplotlib.image as mpimg img=mpimg.imread(&#39;IMG_2557 (1).jpg&#39;) x = img print(x) plt.imshow(x) . [[[ 41 94 172] [ 37 90 168] [ 28 81 159] ... [ 27 79 90] [ 48 94 107] [ 79 118 133]] [[ 29 82 160] [ 40 93 171] [ 43 96 174] ... [ 29 81 92] [ 46 92 105] [ 71 110 125]] [[ 25 81 158] [ 47 100 178] [ 56 109 187] ... [ 27 79 92] [ 42 88 103] [ 60 102 118]] ... [[ 32 12 21] [ 28 13 20] [ 23 12 18] ... [ 13 13 15] [ 15 15 17] [ 20 20 22]] [[ 41 15 26] [ 36 13 23] [ 28 11 19] ... [ 12 12 14] [ 16 16 18] [ 21 21 23]] [[ 53 23 35] [ 46 18 30] [ 33 13 22] ... [ 12 12 14] [ 16 16 18] [ 23 23 25]]] . &lt;matplotlib.image.AxesImage at 0x7f8cd219ae50&gt; . from mpl_toolkits.mplot3d import Axes3D X = np.arange(-5, 5, 0.25) Y = np.arange(-5, 5, 0.25) X, Y = np.meshgrid(X, Y) Z = np.sin(np.sqrt(X**2 + Y**2)) fig = plt.figure() ax = Axes3D(fig) ax.plot_surface(X, Y, Z, cmap=&#39;hot&#39;) ax.set_xlabel(&#39;X values&#39;) ax.set_ylabel(&#39;Y values&#39;) ax.set_zlabel(&#39;Z values&#39;) plt.title(&quot;3D Surface Plot&quot;) plt.show() .",
            "url": "https://suuiq.github.io/blue/2022/03/30/matplotlib.html",
            "relUrl": "/2022/03/30/matplotlib.html",
            "date": " • Mar 30, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "3.3 특성 공학과 규제",
            "content": "import pandas as pd import numpy as np from sklearn.model_selection import train_test_split df = pd.read_csv(&#39;https://bit.ly/perch_csv_data&#39;) perch_full = df.to_numpy() print(perch_full) . [[ 8.4 2.11 1.41] [13.7 3.53 2. ] [15. 3.82 2.43] [16.2 4.59 2.63] [17.4 4.59 2.94] [18. 5.22 3.32] [18.7 5.2 3.12] [19. 5.64 3.05] [19.6 5.14 3.04] [20. 5.08 2.77] [21. 5.69 3.56] [21. 5.92 3.31] [21. 5.69 3.67] [21.3 6.38 3.53] [22. 6.11 3.41] [22. 5.64 3.52] [22. 6.11 3.52] [22. 5.88 3.52] [22. 5.52 4. ] [22.5 5.86 3.62] [22.5 6.79 3.62] [22.7 5.95 3.63] [23. 5.22 3.63] [23.5 6.28 3.72] [24. 7.29 3.72] [24. 6.38 3.82] [24.6 6.73 4.17] [25. 6.44 3.68] [25.6 6.56 4.24] [26.5 7.17 4.14] [27.3 8.32 5.14] [27.5 7.17 4.34] [27.5 7.05 4.34] [27.5 7.28 4.57] [28. 7.82 4.2 ] [28.7 7.59 4.64] [30. 7.62 4.77] [32.8 10.03 6.02] [34.5 10.26 6.39] [35. 11.49 7.8 ] [36.5 10.88 6.86] [36. 10.61 6.74] [37. 10.84 6.26] [37. 10.57 6.37] [39. 11.14 7.49] [39. 11.14 6. ] [39. 12.43 7.35] [40. 11.93 7.11] [40. 11.73 7.22] [40. 12.38 7.46] [40. 11.14 6.63] [42. 12.8 6.87] [43. 11.93 7.28] [43. 12.51 7.42] [43.5 12.6 8.14] [44. 12.49 7.6 ]] . perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0]) . train_input, test_input, train_target, test_target = train_test_split( perch_full, perch_weight, random_state = 42 ) . &#49324;&#51060;&#53431;&#47088;&#51032; &#48320;&#54872;&#44592; . from sklearn.preprocessing import PolynomialFeatures . poly = PolynomialFeatures() poly.fit([[2,3]]) print(poly.transform([[2,3]])) . [[1. 2. 3. 4. 6. 9.]] . poly = PolynomialFeatures(include_bias=False) poly.fit([[2,3]]) print(poly.transform([[2,3]])) # 절편을 위한 항 제거 , 특성의 제곱, 특성끼리의 곱 . [[2. 3. 4. 6. 9.]] . poly = PolynomialFeatures(include_bias=False) poly.fit(train_input) train_poly = poly.transform(train_input) print(train_poly.shape) . (42, 9) . poly.get_feature_names() . /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead. warnings.warn(msg, category=FutureWarning) . [&#39;x0&#39;, &#39;x1&#39;, &#39;x2&#39;, &#39;x0^2&#39;, &#39;x0 x1&#39;, &#39;x0 x2&#39;, &#39;x1^2&#39;, &#39;x1 x2&#39;, &#39;x2^2&#39;] . test_poly = poly.transform(test_input) . &#45796;&#51473;&#54924;&#44480;&#47784;&#45944; &#54984;&#47144;&#54616;&#44592; . from sklearn.linear_model import LinearRegression lr = LinearRegression() lr.fit(train_poly, train_target) . LinearRegression() . print(lr.score(train_poly, train_target)) . 0.9903183436982124 . print(lr.score(test_poly, test_target)) . 0.9714559911594134 . poly = PolynomialFeatures(degree=5, include_bias=False) poly.fit(train_input) train_poly= poly.transform(train_input) test_poly= poly.transform(test_input) print(train_poly.shape) . (42, 55) . train_poly # 특성의 개수 55개 = 열의 개수 . array([[1.96000000e+01, 5.14000000e+00, 3.04000000e+00, ..., 7.42244501e+02, 4.38992857e+02, 2.59637799e+02], [2.20000000e+01, 5.88000000e+00, 3.52000000e+00, ..., 1.50793507e+03, 9.02709432e+02, 5.40397483e+02], [1.87000000e+01, 5.20000000e+00, 3.12000000e+00, ..., 8.21240709e+02, 4.92744425e+02, 2.95646655e+02], ..., [2.56000000e+01, 6.56000000e+00, 4.24000000e+00, ..., 3.28023719e+03, 2.12015331e+03, 1.37034299e+03], [4.20000000e+01, 1.28000000e+01, 6.87000000e+00, ..., 5.31239245e+04, 2.85126063e+04, 1.53032504e+04], [3.45000000e+01, 1.02600000e+01, 6.39000000e+00, ..., 2.74661189e+04, 1.71060916e+04, 1.06537939e+04]]) . lr.fit(train_poly, train_target) print(lr.score(train_poly, train_target)) . 0.9999999999991097 . print(lr.score(test_poly, test_target)) # 특성의 개수 크게 증가 &gt; 선형 모델 아주 강력 &gt; 훈련 세트 거의 완벽 학습 &gt; 따라서 훈련세트에 과대적합되므로 테스트 세트의 점수는 좋지 않음 . -144.40579242684848 . &#44508;&#51228; . from sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(train_poly) train_scaled = ss.transform(train_poly) test_scaled = ss.transform(test_poly) . 훈련 세트로 학습 변환기를 사용해 테스트 세트까지 변환해야 함! . &#47551;&#51648; &#54924;&#44480; . from sklearn.linear_model import Ridge r = Ridge() r.fit(train_scaled, train_target) print(r.score(train_scaled, train_target)) . 0.9896101671037343 . print(r.score(test_scaled, test_target)) . 0.9790693977615397 . import matplotlib.pyplot as plt train_score = [] test_score = [] # alpha 값을 변경할때마다 score() 메서드의 결과를 저장할 리스트 생성 . alpha_list = [0.001, 0.01, 0.1, 1, 10, 100] for alpha in alpha_list: rr = Ridge(alpha=alpha) rr.fit(train_scaled, train_target) train_score.append(rr.score(train_scaled, train_target)) test_score.append(rr.score(test_scaled, test_target)) . plt.plot(np.log10(alpha_list), train_score) plt.plot(np.log10(alpha_list), test_score) plt.xlabel(&#39;alpha&#39;) plt.ylabel(&#39;R^2&#39;) plt.show() # 왼쪽 : 과대적합/ 오른쪽 : 과소적합 , 가장 적절한 alpha 값 = -1 . ridge = Ridge(alpha=0.1) ridge.fit(train_scaled, train_target) print(ridge.score(train_scaled, train_target)) print(ridge.score(test_scaled, test_target)) . 0.9903815817570366 0.9827976465386926 . &#46972;&#50136; &#54924;&#44480; . from sklearn.linear_model import Lasso ls = Lasso() ls.fit(train_scaled, train_target) print(ls.score(train_scaled, train_target)) . 0.989789897208096 . print(ls.score(test_scaled, test_target)) . 0.9800593698421883 . train_score = [] test_score = [] alpha_list = [0.001, 0.01, 0.1, 1, 10, 100] for alpha in alpha_list : lss = Lasso(alpha=alpha, max_iter=10000) lss.fit(train_scaled, train_target) train_score.append(lss.score(train_scaled, train_target)) test_score.append(lss.score(test_scaled, test_target)) . /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.878e+04, tolerance: 5.183e+02 coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.297e+04, tolerance: 5.183e+02 coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive . plt.plot(np.log10(alpha_list), train_score) plt.plot(np.log10(alpha_list), test_score) plt.xlabel(&#39;alpha&#39;) plt.ylabel(&#39;R^2&#39;) plt.show() # 최적의 알파 값 = 1 . lo=Lasso(alpha=10) lo.fit(train_scaled, train_target) print(lo.score(train_scaled, train_target)) print(lo.score(test_scaled, test_target)) . 0.9888067471131867 0.9824470598706695 . print(np.sum(lo.coef_ == 0)) . 40 . 4.1 &#47196;&#51648;&#49828;&#54001; &#54924;&#44480; . (로지스틱 회귀, 이진 분류 &gt; 클래스 확률 예측, 시그모이드 함수, 소프트맥스 함수) . &#47085;&#53412;&#48177;&#51032; &#54869;&#47456; (k-&#52572;&#44540;&#51217; &#51060;&#50883;&#51012; &#49324;&#50857;&#54616;&#51088;) . fs=pd.read_csv(&#39;https://bit.ly/fish_csv_data&#39;) fs.head() . Species Weight Length Diagonal Height Width . 0 Bream | 242.0 | 25.4 | 30.0 | 11.5200 | 4.0200 | . 1 Bream | 290.0 | 26.3 | 31.2 | 12.4800 | 4.3056 | . 2 Bream | 340.0 | 26.5 | 31.1 | 12.3778 | 4.6961 | . 3 Bream | 363.0 | 29.0 | 33.5 | 12.7300 | 4.4555 | . 4 Bream | 430.0 | 29.0 | 34.0 | 12.4440 | 5.1340 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; pd.unique(fs[&#39;Species&#39;]) . array([&#39;Bream&#39;, &#39;Roach&#39;, &#39;Whitefish&#39;, &#39;Parkki&#39;, &#39;Perch&#39;, &#39;Pike&#39;, &#39;Smelt&#39;], dtype=object) . fs_input=fs[[&#39;Weight&#39;,&#39;Length&#39;,&#39;Diagonal&#39;,&#39;Height&#39;,&#39;Width&#39;]].to_numpy() . fs_input . array([[2.42000e+02, 2.54000e+01, 3.00000e+01, 1.15200e+01, 4.02000e+00], [2.90000e+02, 2.63000e+01, 3.12000e+01, 1.24800e+01, 4.30560e+00], [3.40000e+02, 2.65000e+01, 3.11000e+01, 1.23778e+01, 4.69610e+00], [3.63000e+02, 2.90000e+01, 3.35000e+01, 1.27300e+01, 4.45550e+00], [4.30000e+02, 2.90000e+01, 3.40000e+01, 1.24440e+01, 5.13400e+00], [4.50000e+02, 2.97000e+01, 3.47000e+01, 1.36024e+01, 4.92740e+00], [5.00000e+02, 2.97000e+01, 3.45000e+01, 1.41795e+01, 5.27850e+00], [3.90000e+02, 3.00000e+01, 3.50000e+01, 1.26700e+01, 4.69000e+00], [4.50000e+02, 3.00000e+01, 3.51000e+01, 1.40049e+01, 4.84380e+00], [5.00000e+02, 3.07000e+01, 3.62000e+01, 1.42266e+01, 4.95940e+00], [4.75000e+02, 3.10000e+01, 3.62000e+01, 1.42628e+01, 5.10420e+00], [5.00000e+02, 3.10000e+01, 3.62000e+01, 1.43714e+01, 4.81460e+00], [5.00000e+02, 3.15000e+01, 3.64000e+01, 1.37592e+01, 4.36800e+00], [3.40000e+02, 3.20000e+01, 3.73000e+01, 1.39129e+01, 5.07280e+00], [6.00000e+02, 3.20000e+01, 3.72000e+01, 1.49544e+01, 5.17080e+00], [6.00000e+02, 3.20000e+01, 3.72000e+01, 1.54380e+01, 5.58000e+00], [7.00000e+02, 3.30000e+01, 3.83000e+01, 1.48604e+01, 5.28540e+00], [7.00000e+02, 3.30000e+01, 3.85000e+01, 1.49380e+01, 5.19750e+00], [6.10000e+02, 3.35000e+01, 3.86000e+01, 1.56330e+01, 5.13380e+00], [6.50000e+02, 3.35000e+01, 3.87000e+01, 1.44738e+01, 5.72760e+00], [5.75000e+02, 3.40000e+01, 3.95000e+01, 1.51285e+01, 5.56950e+00], [6.85000e+02, 3.40000e+01, 3.92000e+01, 1.59936e+01, 5.37040e+00], [6.20000e+02, 3.45000e+01, 3.97000e+01, 1.55227e+01, 5.28010e+00], [6.80000e+02, 3.50000e+01, 4.06000e+01, 1.54686e+01, 6.13060e+00], [7.00000e+02, 3.50000e+01, 4.05000e+01, 1.62405e+01, 5.58900e+00], [7.25000e+02, 3.50000e+01, 4.09000e+01, 1.63600e+01, 6.05320e+00], [7.20000e+02, 3.50000e+01, 4.06000e+01, 1.63618e+01, 6.09000e+00], [7.14000e+02, 3.60000e+01, 4.15000e+01, 1.65170e+01, 5.85150e+00], [8.50000e+02, 3.60000e+01, 4.16000e+01, 1.68896e+01, 6.19840e+00], [1.00000e+03, 3.70000e+01, 4.26000e+01, 1.89570e+01, 6.60300e+00], [9.20000e+02, 3.85000e+01, 4.41000e+01, 1.80369e+01, 6.30630e+00], [9.55000e+02, 3.85000e+01, 4.40000e+01, 1.80840e+01, 6.29200e+00], [9.25000e+02, 3.95000e+01, 4.53000e+01, 1.87542e+01, 6.74970e+00], [9.75000e+02, 4.10000e+01, 4.59000e+01, 1.86354e+01, 6.74730e+00], [9.50000e+02, 4.10000e+01, 4.65000e+01, 1.76235e+01, 6.37050e+00], [4.00000e+01, 1.41000e+01, 1.62000e+01, 4.14720e+00, 2.26800e+00], [6.90000e+01, 1.82000e+01, 2.03000e+01, 5.29830e+00, 2.82170e+00], [7.80000e+01, 1.88000e+01, 2.12000e+01, 5.57560e+00, 2.90440e+00], [8.70000e+01, 1.98000e+01, 2.22000e+01, 5.61660e+00, 3.17460e+00], [1.20000e+02, 2.00000e+01, 2.22000e+01, 6.21600e+00, 3.57420e+00], [0.00000e+00, 2.05000e+01, 2.28000e+01, 6.47520e+00, 3.35160e+00], [1.10000e+02, 2.08000e+01, 2.31000e+01, 6.16770e+00, 3.39570e+00], [1.20000e+02, 2.10000e+01, 2.37000e+01, 6.11460e+00, 3.29430e+00], [1.50000e+02, 2.20000e+01, 2.47000e+01, 5.80450e+00, 3.75440e+00], [1.45000e+02, 2.20000e+01, 2.43000e+01, 6.63390e+00, 3.54780e+00], [1.60000e+02, 2.25000e+01, 2.53000e+01, 7.03340e+00, 3.82030e+00], [1.40000e+02, 2.25000e+01, 2.50000e+01, 6.55000e+00, 3.32500e+00], [1.60000e+02, 2.25000e+01, 2.50000e+01, 6.40000e+00, 3.80000e+00], [1.69000e+02, 2.40000e+01, 2.72000e+01, 7.53440e+00, 3.83520e+00], [1.61000e+02, 2.34000e+01, 2.67000e+01, 6.91530e+00, 3.63120e+00], [2.00000e+02, 2.35000e+01, 2.68000e+01, 7.39680e+00, 4.12720e+00], [1.80000e+02, 2.52000e+01, 2.79000e+01, 7.08660e+00, 3.90600e+00], [2.90000e+02, 2.60000e+01, 2.92000e+01, 8.87680e+00, 4.49680e+00], [2.72000e+02, 2.70000e+01, 3.06000e+01, 8.56800e+00, 4.77360e+00], [3.90000e+02, 3.17000e+01, 3.50000e+01, 9.48500e+00, 5.35500e+00], [2.70000e+02, 2.60000e+01, 2.87000e+01, 8.38040e+00, 4.24760e+00], [2.70000e+02, 2.65000e+01, 2.93000e+01, 8.14540e+00, 4.24850e+00], [3.06000e+02, 2.80000e+01, 3.08000e+01, 8.77800e+00, 4.68160e+00], [5.40000e+02, 3.10000e+01, 3.40000e+01, 1.07440e+01, 6.56200e+00], [8.00000e+02, 3.64000e+01, 3.96000e+01, 1.17612e+01, 6.57360e+00], [1.00000e+03, 4.00000e+01, 4.35000e+01, 1.23540e+01, 6.52500e+00], [5.50000e+01, 1.47000e+01, 1.65000e+01, 6.84750e+00, 2.32650e+00], [6.00000e+01, 1.55000e+01, 1.74000e+01, 6.57720e+00, 2.31420e+00], [9.00000e+01, 1.77000e+01, 1.98000e+01, 7.40520e+00, 2.67300e+00], [1.20000e+02, 1.90000e+01, 2.13000e+01, 8.39220e+00, 2.91810e+00], [1.50000e+02, 2.00000e+01, 2.24000e+01, 8.89280e+00, 3.29280e+00], [1.40000e+02, 2.07000e+01, 2.32000e+01, 8.53760e+00, 3.29440e+00], [1.70000e+02, 2.07000e+01, 2.32000e+01, 9.39600e+00, 3.41040e+00], [1.45000e+02, 2.15000e+01, 2.41000e+01, 9.73640e+00, 3.15710e+00], [2.00000e+02, 2.30000e+01, 2.58000e+01, 1.03458e+01, 3.66360e+00], [2.73000e+02, 2.50000e+01, 2.80000e+01, 1.10880e+01, 4.14400e+00], [3.00000e+02, 2.60000e+01, 2.90000e+01, 1.13680e+01, 4.23400e+00], [5.90000e+00, 8.40000e+00, 8.80000e+00, 2.11200e+00, 1.40800e+00], [3.20000e+01, 1.37000e+01, 1.47000e+01, 3.52800e+00, 1.99920e+00], [4.00000e+01, 1.50000e+01, 1.60000e+01, 3.82400e+00, 2.43200e+00], [5.15000e+01, 1.62000e+01, 1.72000e+01, 4.59240e+00, 2.63160e+00], [7.00000e+01, 1.74000e+01, 1.85000e+01, 4.58800e+00, 2.94150e+00], [1.00000e+02, 1.80000e+01, 1.92000e+01, 5.22240e+00, 3.32160e+00], [7.80000e+01, 1.87000e+01, 1.94000e+01, 5.19920e+00, 3.12340e+00], [8.00000e+01, 1.90000e+01, 2.02000e+01, 5.63580e+00, 3.05020e+00], [8.50000e+01, 1.96000e+01, 2.08000e+01, 5.13760e+00, 3.03680e+00], [8.50000e+01, 2.00000e+01, 2.10000e+01, 5.08200e+00, 2.77200e+00], [1.10000e+02, 2.10000e+01, 2.25000e+01, 5.69250e+00, 3.55500e+00], [1.15000e+02, 2.10000e+01, 2.25000e+01, 5.91750e+00, 3.30750e+00], [1.25000e+02, 2.10000e+01, 2.25000e+01, 5.69250e+00, 3.66750e+00], [1.30000e+02, 2.13000e+01, 2.28000e+01, 6.38400e+00, 3.53400e+00], [1.20000e+02, 2.20000e+01, 2.35000e+01, 6.11000e+00, 3.40750e+00], [1.20000e+02, 2.20000e+01, 2.35000e+01, 5.64000e+00, 3.52500e+00], [1.30000e+02, 2.20000e+01, 2.35000e+01, 6.11000e+00, 3.52500e+00], [1.35000e+02, 2.20000e+01, 2.35000e+01, 5.87500e+00, 3.52500e+00], [1.10000e+02, 2.20000e+01, 2.35000e+01, 5.52250e+00, 3.99500e+00], [1.30000e+02, 2.25000e+01, 2.40000e+01, 5.85600e+00, 3.62400e+00], [1.50000e+02, 2.25000e+01, 2.40000e+01, 6.79200e+00, 3.62400e+00], [1.45000e+02, 2.27000e+01, 2.42000e+01, 5.95320e+00, 3.63000e+00], [1.50000e+02, 2.30000e+01, 2.45000e+01, 5.21850e+00, 3.62600e+00], [1.70000e+02, 2.35000e+01, 2.50000e+01, 6.27500e+00, 3.72500e+00], [2.25000e+02, 2.40000e+01, 2.55000e+01, 7.29300e+00, 3.72300e+00], [1.45000e+02, 2.40000e+01, 2.55000e+01, 6.37500e+00, 3.82500e+00], [1.88000e+02, 2.46000e+01, 2.62000e+01, 6.73340e+00, 4.16580e+00], [1.80000e+02, 2.50000e+01, 2.65000e+01, 6.43950e+00, 3.68350e+00], [1.97000e+02, 2.56000e+01, 2.70000e+01, 6.56100e+00, 4.23900e+00], [2.18000e+02, 2.65000e+01, 2.80000e+01, 7.16800e+00, 4.14400e+00], [3.00000e+02, 2.73000e+01, 2.87000e+01, 8.32300e+00, 5.13730e+00], [2.60000e+02, 2.75000e+01, 2.89000e+01, 7.16720e+00, 4.33500e+00], [2.65000e+02, 2.75000e+01, 2.89000e+01, 7.05160e+00, 4.33500e+00], [2.50000e+02, 2.75000e+01, 2.89000e+01, 7.28280e+00, 4.56620e+00], [2.50000e+02, 2.80000e+01, 2.94000e+01, 7.82040e+00, 4.20420e+00], [3.00000e+02, 2.87000e+01, 3.01000e+01, 7.58520e+00, 4.63540e+00], [3.20000e+02, 3.00000e+01, 3.16000e+01, 7.61560e+00, 4.77160e+00], [5.14000e+02, 3.28000e+01, 3.40000e+01, 1.00300e+01, 6.01800e+00], [5.56000e+02, 3.45000e+01, 3.65000e+01, 1.02565e+01, 6.38750e+00], [8.40000e+02, 3.50000e+01, 3.73000e+01, 1.14884e+01, 7.79570e+00], [6.85000e+02, 3.65000e+01, 3.90000e+01, 1.08810e+01, 6.86400e+00], [7.00000e+02, 3.60000e+01, 3.83000e+01, 1.06091e+01, 6.74080e+00], [7.00000e+02, 3.70000e+01, 3.94000e+01, 1.08350e+01, 6.26460e+00], [6.90000e+02, 3.70000e+01, 3.93000e+01, 1.05717e+01, 6.36660e+00], [9.00000e+02, 3.90000e+01, 4.14000e+01, 1.11366e+01, 7.49340e+00], [6.50000e+02, 3.90000e+01, 4.14000e+01, 1.11366e+01, 6.00300e+00], [8.20000e+02, 3.90000e+01, 4.13000e+01, 1.24313e+01, 7.35140e+00], [8.50000e+02, 4.00000e+01, 4.23000e+01, 1.19286e+01, 7.10640e+00], [9.00000e+02, 4.00000e+01, 4.25000e+01, 1.17300e+01, 7.22500e+00], [1.01500e+03, 4.00000e+01, 4.24000e+01, 1.23808e+01, 7.46240e+00], [8.20000e+02, 4.00000e+01, 4.25000e+01, 1.11350e+01, 6.63000e+00], [1.10000e+03, 4.20000e+01, 4.46000e+01, 1.28002e+01, 6.86840e+00], [1.00000e+03, 4.30000e+01, 4.52000e+01, 1.19328e+01, 7.27720e+00], [1.10000e+03, 4.30000e+01, 4.55000e+01, 1.25125e+01, 7.41650e+00], [1.00000e+03, 4.35000e+01, 4.60000e+01, 1.26040e+01, 8.14200e+00], [1.00000e+03, 4.40000e+01, 4.66000e+01, 1.24888e+01, 7.59580e+00], [2.00000e+02, 3.23000e+01, 3.48000e+01, 5.56800e+00, 3.37560e+00], [3.00000e+02, 3.40000e+01, 3.78000e+01, 5.70780e+00, 4.15800e+00], [3.00000e+02, 3.50000e+01, 3.88000e+01, 5.93640e+00, 4.38440e+00], [3.00000e+02, 3.73000e+01, 3.98000e+01, 6.28840e+00, 4.01980e+00], [4.30000e+02, 3.80000e+01, 4.05000e+01, 7.29000e+00, 4.57650e+00], [3.45000e+02, 3.85000e+01, 4.10000e+01, 6.39600e+00, 3.97700e+00], [4.56000e+02, 4.25000e+01, 4.55000e+01, 7.28000e+00, 4.32250e+00], [5.10000e+02, 4.25000e+01, 4.55000e+01, 6.82500e+00, 4.45900e+00], [5.40000e+02, 4.30000e+01, 4.58000e+01, 7.78600e+00, 5.12960e+00], [5.00000e+02, 4.50000e+01, 4.80000e+01, 6.96000e+00, 4.89600e+00], [5.67000e+02, 4.60000e+01, 4.87000e+01, 7.79200e+00, 4.87000e+00], [7.70000e+02, 4.80000e+01, 5.12000e+01, 7.68000e+00, 5.37600e+00], [9.50000e+02, 5.17000e+01, 5.51000e+01, 8.92620e+00, 6.17120e+00], [1.25000e+03, 5.60000e+01, 5.97000e+01, 1.06863e+01, 6.98490e+00], [1.60000e+03, 6.00000e+01, 6.40000e+01, 9.60000e+00, 6.14400e+00], [1.55000e+03, 6.00000e+01, 6.40000e+01, 9.60000e+00, 6.14400e+00], [1.65000e+03, 6.34000e+01, 6.80000e+01, 1.08120e+01, 7.48000e+00], [6.70000e+00, 9.80000e+00, 1.08000e+01, 1.73880e+00, 1.04760e+00], [7.50000e+00, 1.05000e+01, 1.16000e+01, 1.97200e+00, 1.16000e+00], [7.00000e+00, 1.06000e+01, 1.16000e+01, 1.72840e+00, 1.14840e+00], [9.70000e+00, 1.10000e+01, 1.20000e+01, 2.19600e+00, 1.38000e+00], [9.80000e+00, 1.12000e+01, 1.24000e+01, 2.08320e+00, 1.27720e+00], [8.70000e+00, 1.13000e+01, 1.26000e+01, 1.97820e+00, 1.28520e+00], [1.00000e+01, 1.18000e+01, 1.31000e+01, 2.21390e+00, 1.28380e+00], [9.90000e+00, 1.18000e+01, 1.31000e+01, 2.21390e+00, 1.16590e+00], [9.80000e+00, 1.20000e+01, 1.32000e+01, 2.20440e+00, 1.14840e+00], [1.22000e+01, 1.22000e+01, 1.34000e+01, 2.09040e+00, 1.39360e+00], [1.34000e+01, 1.24000e+01, 1.35000e+01, 2.43000e+00, 1.26900e+00], [1.22000e+01, 1.30000e+01, 1.38000e+01, 2.27700e+00, 1.25580e+00], [1.97000e+01, 1.43000e+01, 1.52000e+01, 2.87280e+00, 2.06720e+00], [1.99000e+01, 1.50000e+01, 1.62000e+01, 2.93220e+00, 1.87920e+00]]) . fs_input[:5] . array([[242. , 25.4 , 30. , 11.52 , 4.02 ], [290. , 26.3 , 31.2 , 12.48 , 4.3056], [340. , 26.5 , 31.1 , 12.3778, 4.6961], [363. , 29. , 33.5 , 12.73 , 4.4555], [430. , 29. , 34. , 12.444 , 5.134 ]]) . fs_target = fs[&#39;Species&#39;].to_numpy() . train_input, test_input, train_target, test_target = train_test_split(fs_input, fs_target, random_state=42) . sc = StandardScaler() sc.fit(train_input) train_scaled = sc.transform(train_input) test_scaled = sc.transform(test_input) . k-&#52572;&#44540;&#51217; &#51060;&#50883; &#48516;&#47448;&#44592;&#51032; &#54869;&#47456; &#50696;&#52769; . from sklearn.neighbors import KNeighborsClassifier kn=KNeighborsClassifier(n_neighbors=3) kn.fit(train_scaled, train_target) #훈련 세트로 모델을 훈련 print(kn.score(train_scaled, train_target)) print(kn.score(test_scaled, test_target)) . 0.8907563025210085 0.85 . print(kn.classes_) . [&#39;Bream&#39; &#39;Parkki&#39; &#39;Perch&#39; &#39;Pike&#39; &#39;Roach&#39; &#39;Smelt&#39; &#39;Whitefish&#39;] . print(kn.predict(test_scaled[:5])) . [&#39;Perch&#39; &#39;Smelt&#39; &#39;Pike&#39; &#39;Perch&#39; &#39;Perch&#39;] . pr=kn.predict_proba(test_scaled[:5]) print(np.round(pr, decimals=4)) . [[0. 0. 1. 0. 0. 0. 0. ] [0. 0. 0. 0. 0. 1. 0. ] [0. 0. 0. 1. 0. 0. 0. ] [0. 0. 0.6667 0. 0.3333 0. 0. ] [0. 0. 0.6667 0. 0.3333 0. 0. ]] . distances, indexes = kn.kneighbors(test_scaled[3:4]) print(train_target[indexes]) . [[&#39;Roach&#39; &#39;Perch&#39; &#39;Perch&#39;]] . 3개의 최근접 이웃을 사용하기 때문에 가능한 확률이 부족하다 . &#47196;&#51648;&#49828;&#54001; &#54924;&#44480; . z = np.arange(-5, 5, 0.1) phi = 1/(1+np.exp(-z)) plt.plot(z, phi) plt.xlabel(&#39;z&#39;) plt.ylabel(&#39;phi&#39;) plt.show() . 시그모이드 함수의 출력값이 0.5보다 크면 양성, 작으면 음성 &gt; 이진분류 수행 . br_sm_indexes = (train_target == &#39;Bream&#39;) | (train_target == &#39;Smelt&#39;) train_br_sm = train_scaled[br_sm_indexes] target_br_sm = train_target[br_sm_indexes] . from sklearn.linear_model import LogisticRegression lr = LogisticRegression() lr.fit(train_br_sm, target_br_sm) . LogisticRegression() . print(lr.predict(train_br_sm[:5])) . [&#39;Bream&#39; &#39;Smelt&#39; &#39;Bream&#39; &#39;Bream&#39; &#39;Bream&#39;] . print(lr.predict_proba(train_br_sm[:5])) . [[0.99759855 0.00240145] [0.02735183 0.97264817] [0.99486072 0.00513928] [0.98584202 0.01415798] [0.99767269 0.00232731]] . print(lr.classes_) . [&#39;Bream&#39; &#39;Smelt&#39;] . print(lr.coef_, lr.intercept_) . [[-0.4037798 -0.57620209 -0.66280298 -1.01290277 -0.73168947]] [-2.16155132] . de=lr.decision_function(train_br_sm[:5]) print(de) . [-6.02927744 3.57123907 -5.26568906 -4.24321775 -6.0607117 ] . from scipy.special import expit print(expit(de)) . [0.00240145 0.97264817 0.00513928 0.01415798 0.00232731] . lr = LogisticRegression(C=20, max_iter=1000) lr.fit(train_scaled, train_target) print(lr.score(train_scaled, train_target)) print(lr.score(test_scaled, test_target)) . 0.9327731092436975 0.925 . print(lr.predict(test_scaled[:5])) . [&#39;Perch&#39; &#39;Smelt&#39; &#39;Pike&#39; &#39;Roach&#39; &#39;Perch&#39;] . pr=lr.predict_proba(test_scaled[:5]) print(np.round(pr, decimals=3)) . [[0. 0.014 0.841 0. 0.136 0.007 0.003] [0. 0.003 0.044 0. 0.007 0.946 0. ] [0. 0. 0.034 0.935 0.015 0.016 0. ] [0.011 0.034 0.306 0.007 0.567 0. 0.076] [0. 0. 0.904 0.002 0.089 0.002 0.001]] . print(lr.coef_.shape, lr.intercept_.shape) . (7, 5) (7,) . dc=lr.decision_function(test_scaled[:5]) print(np.round(dc, decimals=2)) . [[ -6.5 1.03 5.16 -2.73 3.34 0.33 -0.63] [-10.86 1.93 4.77 -2.4 2.98 7.84 -4.26] [ -4.34 -6.23 3.17 6.49 2.36 2.42 -3.87] [ -0.68 0.45 2.65 -1.19 3.26 -5.75 1.26] [ -6.4 -1.99 5.82 -0.11 3.5 -0.11 -0.71]] . from scipy.special import softmax proba=softmax(dc, axis=1) print(np.round(proba, decimals=3)) . [[0. 0.014 0.841 0. 0.136 0.007 0.003] [0. 0.003 0.044 0. 0.007 0.946 0. ] [0. 0. 0.034 0.935 0.015 0.016 0. ] [0.011 0.034 0.306 0.007 0.567 0. 0.076] [0. 0. 0.904 0.002 0.089 0.002 0.001]] . 4.2 &#54869;&#47456;&#51201; &#44221;&#49324; &#54616;&#44053;&#48277; . fh = pd.read_csv(&#39;https://bit.ly/fish_csv_data&#39;) . fh_input = fh[[&#39;Weight&#39;,&#39;Length&#39;,&#39;Diagonal&#39;,&#39;Height&#39;,&#39;Width&#39;]].to_numpy() fh_target = fh[&#39;Species&#39;].to_numpy() . train_input, test_input, train_target, test_target = train_test_split(fh_input, fh_target, random_state=42) . ss.fit(train_input) train_scaled = ss.transform(train_input) test_scaled = ss.transform(test_input) . from sklearn.linear_model import SGDClassifier . sc=SGDClassifier(loss=&#39;log&#39;, max_iter=10, random_state=42) sc.fit(train_scaled, train_target) print(sc.score(train_scaled, train_target)) print(sc.score(test_scaled, test_target)) . 0.773109243697479 0.775 . /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:700: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit. ConvergenceWarning, . sc.partial_fit(train_scaled, train_target) print(sc.score(train_scaled, train_target)) print(sc.score(test_scaled, test_target)) . 0.8151260504201681 0.85 . sc=SGDClassifier(loss=&#39;log&#39;, random_state=42) train_score = [] test_score = [] classes = np.unique(train_target) . for _ in range(0, 300): sc.partial_fit(train_scaled, train_target, classes = classes) train_score.append(sc.score(train_scaled, train_target)) test_score.append(sc.score(test_scaled, test_target)) . plt.plot(train_score) plt.plot(test_score) plt.xlabel(&#39;epoch&#39;) plt.ylabel(&#39;accuracy&#39;) plt.show() . sc=SGDClassifier(loss=&#39;log&#39;, max_iter=100, tol=None, random_state=42) sc.fit(train_scaled, train_target) print(sc.score(train_scaled, train_target)) print(sc.score(test_scaled, test_target)) . 0.957983193277311 0.925 . sc=SGDClassifier(loss=&#39;hinge&#39;, max_iter=100, tol=None, random_state=42) sc.fit(train_scaled, train_target) print(sc.score(train_scaled, train_target)) print(sc.score(test_scaled, test_target)) . 0.9495798319327731 0.925 .",
            "url": "https://suuiq.github.io/blue/2022/03/30/_03_31.html",
            "relUrl": "/2022/03/30/_03_31.html",
            "date": " • Mar 30, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "2.2 데이터 전처리",
            "content": "import numpy as np . fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9] . fish_data = np.column_stack((fish_length, fish_weight)) . np.column_stack(([1,2,3],[4,5,6])) . array([[1, 4], [2, 5], [3, 6]]) . 각각 원하는 개수의 1, 0을 채운 배열을 만들어주는 함수 : np.ones(), np.zeros() | . print(np.ones(5)) . [1. 1. 1. 1. 1.] . 첫번째 차원을 따라 배열을 연결하는 함수 : np.concatenate() | . fish_target = np.concatenate((np.ones(35),np.zeros(14))) . print(fish_target) . [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] . &#49324;&#51060;&#53431;&#47088;&#51004;&#47196; &#54984;&#47144; &#49464;&#53944;&#50752; &#53580;&#49828;&#53944; &#49464;&#53944; &#45208;&#45572;&#44592; . 전달되는 리스트나 배열을 비율에 맞게 훈련 세트와 테스트 세트로 나눠주는 함수 : train_test_split() | . from sklearn.model_selection import train_test_split . train_input, test_input, train_target, test_target = train_test_split(fish_data, fish_target, random_state=42) . print(train_input.shape, test_input.shape) # 입력 데이터 &gt; 2개의 열이 있는 2차원 배열 . (36, 2) (13, 2) . print(train_target.shape, test_target.shape) # 타깃 데이터 &gt; 1차원 배열 . (36,) (13,) . train_input, test_input, train_target, test_target = train_test_split(fish_data, fish_target, stratify=fish_target, random_state=42) . print(test_target) # 비율 맞춰진거 확인 가능 . [0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1.] . &#49688;&#49345;&#54620; &#46020;&#48120; &#54620; &#47560;&#47532; . from sklearn.neighbors import KNeighborsClassifier kn = KNeighborsClassifier() kn.fit(train_input, train_target) kn.score(test_input, test_target) . 1.0 . print(kn.predict([[25, 150]])) . [0.] . import matplotlib.pyplot as plt . 산점도 그래프를 그려보면 도미 데이터에 더 가까움에도 불구하고 예측값은 0이 나옴 &gt; kneighbors() 이용하면 이웃까지의 거리, 샘플의 인덱스 반환 가능 . distances, indexes = kn.kneighbors([[25,150]]) . plt.scatter(train_input[:,0], train_input[:,1]) plt.scatter(25, 150, marker = &#39;^&#39;) plt.scatter(train_input[indexes,0], train_input[indexes,1], marker=&#39;D&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) . Text(0, 0.5, &#39;weight&#39;) . 가장 가까운 데이터에 도미가 1개뿐! 나머지 4개의 샘플 모두 빙어 . print(train_target[indexes]) . [[1. 0. 0. 0. 0.]] . 산점도를 보면 도미와 가까워 보이는데.. 왜 가장 가까운 이웃이 빙어가 4개나 될까? &gt; distances를 이용해 이웃 샘플까지의 거리를 알아보자 . print(distances) . [[ 92.00086956 130.48375378 130.73859415 138.32150953 138.39320793]] . x축의 범위는 (10-40), y축의 범위는 (0-1000) &gt; 따라서 y축 방향으로 약간 먼 도미가 아주 큰 값으로 계산됨 &gt; x축의 범위를 동일하게 맞춰보자 . plt.scatter(train_input[:,0], train_input[:,1]) plt.scatter(25, 150, marker = &#39;^&#39;) plt.scatter(train_input[indexes,0], train_input[indexes,1], marker=&#39;D&#39;) plt.xlim((0,1000)) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) . Text(0, 0.5, &#39;weight&#39;) . 샘플 간의 거리에 영향을 많이 받으므로 제대로 사용하기 위해서는 특성값을 일정한 기준으로 맞춰야 함 &gt; 데이터 전처리 과정 . &#54364;&#51456;&#51216;&#49688; . mean = np.mean(train_input, axis=0) std = np.std(train_input, axis=0) # axis=0 &gt; 각 특성별로 평균, 표준편차 계산 . train_scaled = (train_input - mean)/std # 브로드캐스팅 : 모든 행에서 처리 작용 . &#51204;&#52376;&#47532; &#45936;&#51060;&#53552;&#47196; &#47784;&#45944; &#54984;&#47144;&#54616;&#44592; . plt.scatter(train_scaled[:,0], train_scaled[:,1]) plt.scatter(25, 150, marker = &#39;^&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() . 도미 샘플 [25,150] 또한 동일한 비율로 변환해야 함! . new = ([25,150] - mean)/std . kn.fit(train_scaled, train_target) . KNeighborsClassifier() . test_scaled = (test_input - mean)/std . kn.score(test_scaled, test_target) . 1.0 . print(kn.predict([new])) . [1.] . distances, indexes = kn.kneighbors([new]) plt.scatter(train_scaled[:,0], train_scaled[:,1]) plt.scatter(new[0], new[1], marker=&#39;^&#39;) plt.scatter(train_scaled[indexes, 0], train_scaled[indexes,1], marker=&#39;D&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() . 3.1 k-&#52572;&#44540;&#51217; &#51060;&#50883; &#54924;&#44480; (&#45453;&#50612;&#51032; &#47924;&#44172;&#47484; &#50696;&#52769;&#54644;&#48372;&#51088;) . import numpy as np perch_length = np.array([8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0, 21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7, 23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5, 27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5, 44.0]) perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0, 1000.0]) . train_input, test_input, train_target, test_target = train_test_split(perch_length, perch_weight, random_state=42) . ! 사이킷런에 사용할 훈련 세트는 2차원 배열이어야 한다, 하지만 perch_length는 1차원 배열이므로 1개의 열이 있는 2차원 배열로 변경해야함 . train_input = train_input.reshape(-1,1) test_input = test_input.reshape(-1,1) print(train_input.shape, test_input.shape) . (42, 1) (14, 1) . &#44208;&#51221;&#44228;&#49688; . from sklearn.neighbors import KNeighborsRegressor knr = KNeighborsRegressor() knr.fit(train_input, train_target) . KNeighborsRegressor() . print(knr.score(train_input, train_target)) . 0.9698823289099254 . 과대적합: 훈련세트의 점수 &gt; 테스트 세트 . | 과소적합: 훈련세트 &lt; 테스트세트 or 두점수가 모두 낮은 경우 . | . knr.n_neighbors = 3 knr.fit(train_input, train_target) print(knr.score(train_input, train_target)) . 0.9804899950518966 . 과소적합 해결! . 3.2 &#49440;&#54805; &#54924;&#44480; . k-&#52572;&#44540;&#51217; &#51060;&#50883;&#51032; &#54620;&#44228; . print(knr.predict([[50]])) . [1033.33333333] . distances, indexes = knr.kneighbors([[50]]) plt.scatter(train_input, train_target) plt.scatter(train_input[indexes], train_target[indexes], marker=&#39;D&#39;) plt.scatter(50,1033, marker=&#39;^&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() . 50cm 농어에서 가장 가까운 것이 45cm 근방이라 이 샘플들의 무게를 평균화함 &gt; 따라서 무게 더 적게 산출 / 100cm 농어를 넣어도 똑같은 값이 나옴 . &#49440;&#54805;&#54924;&#44480; . from sklearn.linear_model import LinearRegression lr = LinearRegression() lr.fit(train_input, train_target) print(lr.predict([[50]])) . [1241.83860323] . print(lr.coef_, lr.intercept_) # coef_, intercept_ &gt; 모델 파라미터 . [39.01714496] -709.0186449535477 . plt.scatter(train_input, train_target) plt.plot([15,50], [15*lr.coef_ + lr.intercept_, 50*lr.coef_ + lr.intercept_]) plt.scatter(50, 1241.8, marker=&#39;^&#39;) plt.xlabel(&#39;length&#39;) plt.ylabel(&#39;weight&#39;) plt.show() . print(lr.score(train_input, train_target)) print(lr.score(test_input, test_target)) # 전체적으로 과소적합 and 그래프의 왼쪽 아래 0으로 향함 . 0.939846333997604 0.8247503123313558 .",
            "url": "https://suuiq.github.io/blue/2022/03/30/_03_24.html",
            "relUrl": "/2022/03/30/_03_24.html",
            "date": " • Mar 30, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://suuiq.github.io/blue/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://suuiq.github.io/blue/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "전북대학교 통계학과 3학년 .",
          "url": "https://suuiq.github.io/blue/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://suuiq.github.io/blue/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}